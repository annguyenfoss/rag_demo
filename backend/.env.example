# =============================================================================
# Backend Configuration Example
# =============================================================================
# Copy this file to .env and update the values as needed
# This file can be committed to version control
# The actual .env file should NEVER be committed (it's in .gitignore)
# =============================================================================

# -----------------------------------------------------------------------------
# Deployment Mode Configuration
# -----------------------------------------------------------------------------
# Select deployment mode: 'gpu' (Ollama) or 'wrapper' (OpenAI)
DEPLOYMENT_MODE=gpu

# -----------------------------------------------------------------------------
# OpenAI Configuration (required for wrapper mode)
# -----------------------------------------------------------------------------
# OpenAI API key (required if DEPLOYMENT_MODE=wrapper)
OPENAI_API_KEY=

# OpenAI model to use for chat/completions
OPENAI_MODEL=gpt-4o

# OpenAI embedding model to use for vector embeddings
OPENAI_EMBED_MODEL=text-embedding-3-large

# -----------------------------------------------------------------------------
# Ollama LLM Configuration (used in gpu mode)
# -----------------------------------------------------------------------------
# Base URL for the Ollama API server
OLLAMA_BASE_URL=http://localhost:11434

# The Ollama model to use for chat/completions
OLLAMA_MODEL=qwen3:30b

# The embedding model to use for vector embeddings
EMBED_MODEL=nomic-embed-text

# -----------------------------------------------------------------------------
# Backend Server Configuration
# -----------------------------------------------------------------------------
# Host address for the backend API server
HOST=0.0.0.0

# Port for the backend API server
PORT=9100

# -----------------------------------------------------------------------------
# API Configuration
# -----------------------------------------------------------------------------
# Comma-separated list of allowed CORS origins for the API
ALLOWED_ORIGINS=http://localhost:9200,http://127.0.0.1:9200

# -----------------------------------------------------------------------------
# Vector Database (Milvus) Configuration
# -----------------------------------------------------------------------------
# URI for Milvus database (file path for Milvus Lite, or server URL)
# Default: milvus.db (relative to backend directory)
MILVUS_URI=milvus.db

# Collection name in Milvus to store embeddings
# Default: rag_collection
MILVUS_COLLECTION=rag_collection

# -----------------------------------------------------------------------------
# Storage Configuration
# -----------------------------------------------------------------------------
# Directory for LlamaIndex storage (docstore, index store, etc.)
# Default: storage (relative to backend directory)
STORAGE_DIR=storage

# Directory containing source documents to ingest
# Default: data (relative to backend directory)
DATA_DIR=data
